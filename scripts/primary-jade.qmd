---
title: "Summary of exploratory tasks"
author: "Jade O'Brien"
date: today
---

```{r}
library(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
require(tidyverse)
require(keras)
require(tensorflow)

library(dplyr)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
library(word2vec)
library(tm)
library(kernlab)
```

```{r}
# path to activity files on repo
url <- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/activities/data/'

# load a few functions for the activity
source(paste(url, 'projection-functions.R', sep = ''))

load(here::here("data/claims-raw.RData"))
```

Q1.1
???compare with headers and without headers
```{r}
# function to parse html and clean text (with headers)
parse_fn <- function(.html){
  read_html(.html) %>%
    html_elements('p, h1, h2, h3, h4, h5, h6') %>%
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}


# function to apply to claims data
parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp)) %>%
    unnest(text_clean) 
  return(out)
}

nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'words',
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_words(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}

#clean data set
claims_clean <- parse_data(claims_raw)
claims_clean_tokens <- nlp_fn(claims_clean)
save(claims_clean, file = here::here("data/clean1.RData"))
```


# Splits and Folds
```{r}
load(here::here("data/clean1.RData"))
set.seed(3435)

bc_split <- initial_split(claims_clean_tokens, prop = 0.8, strata = bclass)

claim_train <- training(bc_split)
claim_test <- testing(bc_split)
set.seed(3435)

bc_folds <- vfold_cv(claim_train, v=5, strata = bclass)
```

Recipe

```{r}
claim_recipe <- recipe(bclass ~ ., data = claim_train) %>% 
  step_rm(.id) %>% 
  step_pca(all_numeric_predictors(), threshold = 0.7) |> 
  step_normalize(all_numeric_predictors())
```


Regularized Linear Model

```{r}
enet_mod <- logistic_reg(penalty = tune(), mixture = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("glmnet")

enet_wkflow <- workflow() %>% 
    add_model(enet_mod) %>% 
    add_recipe(claim_recipe)

enet_grid <- grid_regular(penalty(range = c(0,.5), trans = identity_trans()),
                          mixture(range = c(0,.5)), levels = 10)
```

```{r}
    # Elastic Net 
enet_tune_res <- tune_grid(
  object = enet_wkflow, 
  resamples = bc_folds, 
  grid = enet_grid,
  control = control_grid(verbose=TRUE)
    )

save(enet_tune_res, file = here::here("results/enet_tune_res.rda"))
```

```{r}
load(here::here("results/enet_tune_res.rda"))
autoplot(enet_tune_res)
show_best(enet_tune_res, metric = "roc_auc")
final_log_wkflow <- finalize_workflow(enet_wkflow, show_best(enet_tune_res, metric = "roc_auc")[1,c("penalty", "mixture")])
final_log_fit <- fit(final_log_wkflow, data = claim_train)
save(final_log_fit, file = here::here("results/final_log_fit.rda"))
```

```{r}
test_preds <- augment(final_log_fit, new_data = claim_test)
roc_auc(test_preds, truth = bclass, `.pred_N/A: No relevant content.`)
```


# Bigrams

```{r}
bi_nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'ngrams',
                  n = 2,
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_strings(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}
```

```{r}
claims_clean_tokens_bi <- bi_nlp_fn(claims_clean)

all_preds <- augment(final_log_fit, claims_clean_tokens)

all_preds <- all_preds |> 
  mutate(log_odds_pred = log(`.pred_N/A: No relevant content.`  / `.pred_Relevant claim content` + .01)) |> 
  select(.id, log_odds_pred)

full_clean_claims_bigrams <- inner_join(claims_clean_tokens_bi, all_preds)
save(full_clean_claims_bigrams, file = here::here("data/full_clean_claims_bigrams.rda"))
```


```{r}
load(here::here("data/full_clean_claims_bigrams.rda"))

set.seed(1027)
bi_partitions <- full_clean_claims_bigrams %>% initial_split(prop = 0.8, strata = bclass)

bi_claim_train <- training(bi_partitions)
bi_claim_test <- testing(bi_partitions)
bi_claim_test %>% select(log_odds_pred) %>% head()
set.seed(3435)

bi_bc_folds <- vfold_cv(bi_claim_train, v=5, strata = bclass)

bi_claim_recipe <- recipe(bclass ~ ., data = bi_claim_train) |> 
  step_rm(.id) |> 
  step_nzv(all_predictors()) |>
  step_pca(all_numeric_predictors(), -log_odds_pred, num_comp = 50)

enet_mod <- logistic_reg(penalty = tune(), mixture = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("glmnet")

bi_logistic_wkflow <- workflow() %>% 
    add_model(enet_mod) %>% 
    add_recipe(bi_claim_recipe)

enet_grid <- grid_regular(penalty(range = c(0,.5), trans = identity_trans()),
                          mixture(range = c(0,.5)), levels = 10)
```

```{r}
bi_enet_tune_res <- tune_grid(
  object = bi_logistic_wkflow, 
  resamples = bi_bc_folds, 
  grid = enet_grid,
  control = control_grid(verbose=TRUE)
    )

save(bi_enet_tune_res, file = here::here("results/bi_enet_tune_res.rda"))
```


```{r}
autoplot(bi_enet_tune_res)
show_best(bi_enet_tune_res, metric = "roc_auc")
bi_final_log_wkflow <- finalize_workflow(bi_logistic_wkflow, show_best(bi_enet_tune_res, metric = "roc_auc")[1,c("penalty", "mixture")])
bi_final_log_fit <- fit(bi_final_log_wkflow, data = bi_claim_train)
```

```{r}
save(bi_final_log_fit, file = here::here("results/bi_final_log_fit.rda"))
```

```{r}
bi_preds <- augment(bi_final_log_fit, new_data = bi_claim_test)
roc_auc(bi_preds, truth = bclass, `.pred_N/A: No relevant content.`)
heatmap(bi_preds, truth = bclass, `.pred_N/A: No relevant content.`)
```


# Muticlass

```{r}
load(here::here("data/clean1.RData"))
load(here::here("data/clean_claims_tokens.rda"))

mclass_labels <- claims_clean %>% select(.id, mclass)

tokenized_mclass <- inner_join(clean_claims_tokens, mclass_labels)
```


```{r}
set.seed(3435)

bc_split <- initial_split(tokenized_mclass, prop = 0.8)

claim_train <- training(bc_split)
claim_test <- testing(bc_split)
set.seed(3435)
```

Recipe

```{r}
m_claim_recipe <- recipe(mclass ~ ., data = claim_train) %>% 
  step_rm(.id) %>% 
  step_rm(bclass) %>% 
  step_nzv() %>% 
  step_pca(all_numeric_predictors(), threshold = 0.7) |> 
  step_normalize(all_numeric_predictors())
```


```{r}
enet_mod <- multinom_reg(penalty = 0, mixture = 0) %>% 
    set_mode("classification") %>% 
    set_engine("nnet")

enet_wkflow <- workflow() %>% 
    add_model(enet_mod) %>% 
    add_recipe(m_claim_recipe)

m_final_log_fit <- fit(enet_wkflow, data = claim_train)

save(m_final_log_fit, file = here::here("results/m_final_log_fit.rda"))
```


```{r}
load(here::here("data/clean_claims_bigrams.rda"))
preds <- augment(m_final_log_fit, new_data = tokenized_mclass)
preds <- preds %>% select(.id, `.pred_Potentially unlawful activity`, `.pred_Possible Fatality`, `.pred_Physical Activity`, `.pred_N/A: No relevant content.`)

full_clean_claims_bigrams <- inner_join(clean_claims_bigrams, preds)
full_clean_claims_bigrams <- inner_join(full_clean_claims_bigrams, mclass_labels)
```

```{r}
set.seed(1027)
bi_partitions <- full_clean_claims_bigrams %>% initial_split(prop = 0.8)

bi_claim_train <- training(bi_partitions)
bi_claim_test <- testing(bi_partitions)
set.seed(3435)

bi_bc_folds <- vfold_cv(bi_claim_train, v=5)

m_bi_claim_recipe <- recipe(mclass ~ ., data = bi_claim_train) |> 
  step_rm(.id) |> 
  step_rm(bclass) %>% 
  step_nzv(all_predictors()) |>
  step_pca(all_numeric_predictors(), -c(`.pred_Potentially unlawful activity`, `.pred_Possible Fatality`, `.pred_Physical Activity`, `.pred_N/A: No relevant content.`), num_comp = 50) %>% 
  step_normalize(all_numeric_predictors())

enet_mod <- multinom_reg(penalty = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("nnet")

m_bi_logistic_wkflow <- workflow() %>% 
    add_model(enet_mod) %>% 
    add_recipe(m_bi_claim_recipe)

enet_grid <- grid_regular(penalty(range = c(0,.5), trans = identity_trans()),
                          levels = 10)
```

```{r}
m_bi_enet_tune_res <- tune_grid(
  object = m_bi_logistic_wkflow, 
  resamples = bi_bc_folds, 
  grid = enet_grid,
  control = control_grid(verbose=TRUE)
    )
save(m_bi_enet_tune_res, file = here::here("results/m_bi_enet_tune_res.rda"))
```

```{r}
m_bi_final_log_wkflow <- finalize_workflow(m_bi_logistic_wkflow, show_best(m_bi_enet_tune_res)[1,c("penalty")])
m_bi_final_log_fit <- fit(m_bi_final_log_wkflow, data = bi_claim_train)
save(m_bi_final_log_fit, file = here::here("results/m_bi_final_log_fit.rda"))
```


# Primary Task
Support Vector


```{r}
svm_rbf_spec <- svm_rbf(cost = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("kernlab")

svm_rbf_wkflow <- workflow() %>% 
    add_recipe(claim_recipe) %>% 
    add_model(svm_rbf_spec)


svm_grid <- grid_regular(cost(), levels = 5)
```

```{r}
svm_rbf_res <- tune_grid(svm_rbf_wkflow, 
                         resamples = bc_folds, 
                         grid = svm_grid, 
                         control=control_grid(verbose=TRUE))

save(svm_rbf_res, file = here::here("results/svm_rbf_res.rda"))

```

```{r}
autoplot(svm_rbf_res)
```

## Using trigrams

```{r}
tri_nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'ngrams',
                  n = 3,
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_strings(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}

claims_clean_tokens_tri <- tri_nlp_fn(claims_clean)
```

```{r}
load(here::here("data/full_clean_claims_bigrams.rda"))
load(here::here("results/bi_final_log_fit.rda"))
load(here::here("results/final_log_fit.rda"))

all_preds <- augment(final_log_fit, claims_clean_tokens)

all_preds <- all_preds |> 
  mutate(log_odds_pred = log(`.pred_N/A: No relevant content.`  / `.pred_Relevant claim content` + .01)) |> 
  select(.id, log_odds_pred)

full_clean_claims_tri <- inner_join(claims_clean_tokens_tri, all_preds)

all_preds_bi <- augment(bi_final_log_fit, full_clean_claims_bigrams)

all_preds_bi <- all_preds_bi |> 
  mutate(log_odds_pred_bi = log(`.pred_N/A: No relevant content.`  / `.pred_Relevant claim content` + .01)) |> 
  select(.id, log_odds_pred_bi)

full_clean_claims_trigrams <- inner_join(full_clean_claims_tri, all_preds_bi)

mclass_labels <- claims_clean %>% select(.id, mclass)

full_clean_claims_trigrams <- inner_join(full_clean_claims_trigrams, mclass_labels)

save(full_clean_claims_trigrams, file = here::here("data/full_clean_claims_trigrams.rda"))
```

## Binary Modeling Pipeline

```{r}
load(here::here("data/full_clean_claims_trigrams.rda"))
set.seed(1027)
tri_partitions <- full_clean_claims_trigrams %>% initial_split(prop = 0.8, strata = bclass)

tri_claim_train <- training(tri_partitions)
tri_claim_test <- testing(tri_partitions)


set.seed(3435)

tri_bc_folds <- vfold_cv(tri_claim_train, v=5, strata = bclass)

tri_claim_recipe <- recipe(bclass ~ ., data = tri_claim_train) |> 
  step_rm(.id) |> 
  step_rm(mclass) %>% 
  step_nzv(all_predictors()) |>
  step_pca(all_numeric_predictors(), -log_odds_pred, -log_odds_pred_bi, num_comp = 100) %>% 
  step_normalize(all_numeric_predictors())

svm_spec <- svm_linear(cost = tune()) %>% 
    set_mode("classification") %>% 
    set_engine("kernlab")

tri_svm_linear_wkflow <- workflow() %>% 
    add_recipe(tri_claim_recipe) %>% 
    add_model(svm_spec)


svm_grid <- grid_regular(cost(), levels = 5)
```

```{r}
tri_svm_linear_res <- tune_grid(tri_svm_linear_wkflow, 
                         resamples = tri_bc_folds, 
                         grid = svm_grid, 
                         control=control_grid(verbose=TRUE))

save(tri_svm_linear_res, file = here::here("results/tri_svm_linear_res.rda"))
```

```{r}
autoplot(tri_svm_linear_res)
```

