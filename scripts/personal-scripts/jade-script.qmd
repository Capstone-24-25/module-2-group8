---
title: "Summary of exploratory tasks"
author: "Jade O'Brien"
date: today
---


```{r}
require(tidyverse)
require(tidytext)
require(textstem)
require(rvest)
require(qdapRegex)
require(stopwords)
require(tokenizers)
library(tidymodels)
library(modelr)
library(Matrix)
library(sparsesvd)
library(glmnet)
source(here::here("scripts/preprocessing.R"))
load(here::here("data/claims-raw.RData"))
```
# Question 1

## Processing functions with headers

```{r}
parse_fn_headers <- function(.html){
read_html(.html) %>%
    html_elements('h1') %>%
    html_text2() %>%
    str_c(collapse = ' ') %>%
    rm_url() %>%
    rm_email() %>%
    str_remove_all('\'') %>%
    str_replace_all(paste(c('\n', 
                            '[[:punct:]]', 
                            'nbsp', 
                            '[[:digit:]]', 
                            '[[:symbol:]]'),
                          collapse = '|'), ' ') %>%
    str_replace_all("([a-z])([A-Z])", "\\1 \\2") %>%
    tolower() %>%
    str_replace_all("\\s+", " ")
}

parse_data <- function(.df){
  out <- .df %>%
    filter(str_detect(text_tmp, '<!')) %>%
    rowwise() %>%
    mutate(text_clean = parse_fn(text_tmp),
           headers_clean = parse_fn_headers(text_tmp)) %>%
    unnest(text_clean) |> 
    unnest(headers_clean)
  return(out)
}
```

## Creating Data

```{r}
clean_claims <- parse_data(claims_raw)

clean_claims <- clean_claims |> 
  mutate(text_clean = paste(text_clean, headers_clean)) |> 
  select(.id, bclass, mclass, text_clean)

clean_claims_tokens <- nlp_fn(clean_claims)
```


## Modeling Splits

```{r}
set.seed(1027)
partitions <- clean_claims_tokens %>% initial_split(prop = 0.8)

claim_train <- training(partitions)
claim_test <- testing(partitions)
```


## Tidymodels recipe

```{r}
# find projections based on training data
claim_recipe <- recipe(bclass ~ ., data = claim_train) |> 
  step_rm(.id) |> 
  step_pca(all_numeric(), threshold = .7)
```

## Define model and workflow

```{r}
logistic_mod <- logistic_reg(penalty = 0) %>% 
      set_mode("classification") %>% 
      set_engine("glmnet")

logistic_wkflow <- workflow() %>% 
    add_model(logistic_mod) %>% 
    add_recipe(claim_recipe)
```

## Model Fitting and Evaluation

```{r}
logistic_fit <- fit(logistic_wkflow, data=claim_train)

test_results <- augment(logistic_fit, claim_test)

roc_auc(test_results, truth = bclass, `.pred_N/A: No relevant content.`)
conf_mat(test_results, truth = bclass, 
         .pred_class) %>% 
autoplot(type = "heatmap")
```
# Question 2

## Bigrams

```{r}
bi_nlp_fn <- function(parse_data.out){
  out <- parse_data.out %>% 
    unnest_tokens(output = token, 
                  input = text_clean, 
                  token = 'ngrams',
                  n = 2,
                  stopwords = str_remove_all(stop_words$word, 
                                             '[[:punct:]]')) %>%
    mutate(token.lem = lemmatize_strings(token)) %>%
    filter(str_length(token.lem) > 2) %>%
    count(.id, bclass, token.lem, name = 'n') %>%
    bind_tf_idf(term = token.lem, 
                document = .id,
                n = n) %>%
    pivot_wider(id_cols = c('.id', 'bclass'),
                names_from = 'token.lem',
                values_from = 'tf_idf',
                values_fill = 0)
  return(out)
}
```

```{r}
clean_claims_bigrams <- bi_nlp_fn(clean_claims)
```

## Adding log odds predictions

```{r}
all_preds <- augment(logistic_fit, clean_claims_tokens)

all_preds <- all_preds |> 
  mutate(log_odds_pred = log(`.pred_N/A: No relevant content.`  / `.pred_Relevant claim content` + .01)) |> 
  select(.id, log_odds_pred)

join_clean_claims_bigrams <- inner_join(clean_claims_bigrams, all_preds)

```

```{r}
join_clean_claims_bigrams |>  head()
clean_claims_bigrams %>% head()
```


## Splits, Recipe, and Model Specification

```{r}
set.seed(1027)
bi_partitions <- join_clean_claims_bigrams %>% initial_split(prop = 0.8)

bi_claim_train <- training(bi_partitions)
bi_claim_test <- testing(bi_partitions)

bi_claim_recipe <- recipe(bclass ~ ., data = bi_claim_train) |> 
  step_rm(.id) |> 
  step_pca(all_numeric(), threshold = .7)

logistic_mod <- logistic_reg(penalty = 0) %>% 
      set_mode("classification") %>% 
      set_engine("glmnet")

bi_logistic_wkflow <- workflow() %>% 
    add_model(logistic_mod) %>% 
    add_recipe(bi_claim_recipe)
```


### Model Fitting and evaluation

```{r}
logistic_fit_bigrmas <- fit(bi_logistic_wkflow, data=bi_claim_train)

bi_test_results <- augment(logistic_fit_bigrmas, bi_claim_test)

roc_auc(bi_test_results, truth = bclass, `.pred_N/A: No relevant content.`)
conf_mat(bi_test_results, truth = bclass, 
         .pred_class) %>% 
autoplot(type = "heatmap")
```



```{r}
save(clean_claims_bigrams, file = here::here("data/clean_claims_bigrams.rda"))
save(clean_claims_tokens, file = here::here("data/clean_claims_tokens.rda"))
save(logistic_fit, file = here::here("results/header_logistic_tokens_fit.rda"))
save(join_clean_claims_bigrams, file = here::here("data/join_clean_claims_bigrams.rda"))
```

```{r}
load(here::here("data/clean_claims_bigrams.rda"))
load(here::here("data/clean_claims_tokens.rda"))
load(here::here("results/header_logistic_tokens_fit.rda"))
load(here::here("data/join_clean_claims_bigrams.rda"))
```

