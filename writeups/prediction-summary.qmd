---
title: "Predictive modeling of claims status"
author: 'Joshua Charfauros, Jade OBrien, Caitlyn Vasquez, Shirley Wang'
date: today
---

### Abstract

Our primary task was to develop a binary classification model to predict whether or not a webpage has useful information on it, and also a multi-class prediction model to predict the sort of information that would be present on a webpage. Header and paragraph content was scraped from the raw webpages and processed into term frequencies of word tokens. For binary classification, an elastic net regularized logistic regression yielded an accuracy of ~81%, and for our multiclass classification, a multinomial logistic regression gave ~80% accuracy. We also experimented with neural networks to and see which modeling technique yields a higher accuracy. However, we found that the accuracy was about the same. For binary classification, a two-layer neural network gave ~82% accuracy; for multi-class classification, a two-layer neural network gave ~80% accuracy. 

### Preprocessing

 For our data preprocessing, we parse our data using the parse_data() function which cleans our data frame and produces a new column text_clean of just our clean text. parse_data() uses the parse_fn() function that processes our raw HTML and converts it to clean text. We edited the provided version of parse_fn() to also include the header elements. parse_fn() also does a series of string cleaning processes to output clean usable text. Lastly, we tokenize using the nlp_fn() function to split the cleaned text into individual word tokens. We also remove stopwords like “the” or “and” etcetera, since these words are much too common in English and are not useful for modeling. The last notable part of our preprocessing is computing a tf-idf for each token that highlights words which are important in specific documents but not necessarily across the whole data set. This tf-idf is helpful to identify keywords that may indicate a certain class or another.


### Methods



For the binary classification model we used an elastic net with regularized logistic regression. Our predictors for this model were predicted log odds for each HTML document as well as bigram tf-idfs. For our hyperparameters we used 10 grid levels with penalty and mixture ranging from 0-.5. For training, we used 5-fold cross validation.

For the multiclass model, we first ran a multinomial regression on our word token data, extracted the resulting predicted probabilities for each class, and then fed these in as predictors into another regularized logistic regression model. The other predictors of this logistic regression model were bigram tf-idfs. For our hyperparameters we used 10 grid levels with penalty and mixture ranging from 0-.5. We also used 5-fold cross-validation for our training method.

The other modeling technique we used was a two-layer neural network for both the binary classification and multi-class classification. For the binary classification, there is a single hidden layer with 10 units and one output layer with 1 unit, using sigmoid activation. The model is trained using backpropagation through the layers to adjust weights based on the binary cross-entropy loss. We used “Adam” as the optimizer and binary accuracy as the evaluation metric. The training is done over 20 epochs with a validation split of 0.2. The multi-class model has one hidden layer with 64 units (a larger number of units allows us to capture more complex patterns) and one output layer with 5 units, corresponding to the 5 classes, with softmax activation.  Like the binary model, the multi-class model is trained using backpropagation with the Adam optimizer. The sparse categorical cross-entropy is used as the loss function. A validation split of 0.2 is used to evaluate the model’s performance during training and the model is trained for 8 epochs. 

### Results

Predictive accuracy for binary model (elastic net regularized logistic regression) and multi-class model (multinomial logistic regression):

```{r}
binary <- c(accuracy = .810, sensitivity = .695, specificity = .910)
multi <- c(accuracy = .800, sensitivity = .665, specificity = .936)

table <- data.frame(
  Metric = c("Accuracy", "Specificity", "Sensitivity"),
  Binary = c(binary["accuracy"], binary["specificity"],binary["sensitivity"]),
  Multiclass = c(multi["accuracy"], multi["specificity"], multi["sensitivity"])
)

rownames(table) <- NULL
print(table)
```

#### Neural network

- Accuracy for binary model: ~82%

- Accuracy for multi-class model: ~80%
