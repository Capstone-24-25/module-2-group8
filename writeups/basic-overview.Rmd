
# Preprocessing / Preliminary Tasks

## Task 1
  just updated preprocessing script to include header tags from HTML files and then make a binary regularized logistic model from it using word tf-ifs
  
  This model has the following metrics:
  
  - Accuracy: 0.786
  - Sensitivity: 0.797
  - Specificity: 0.776
  - AUC: 0.851
  
  
## Task 2
  
Using the model from task 1 we extract the predicted log odds of each document and use it as a feature in a model where we used bigrams tf-idfs as the other features.
  
  This model has the following metrics:
  
  - Accuracy: 0.81
  - Sensitivity: 0.695
  - Specificity: 0.910
  - AUC: 0.909
  
It's interesting to note that this model has a lower sensitivity than the model in task 1 and I have no idea why
  
# Primary Task
  
## Binary Classifier
  
The best model I could find was the EXACT same model from task 2 with no additional changes. So it has the same metrics as in task 2.
  
I also tried a SVM on bigram data, as well as a SVM on trigram data but they both performed either worse or just about the same.
  

## Multiclass model
  
I adapted the model used in the binary case, so first we run a multinomial logistic regression on word token data, we extract the resulting predicted probabilities for each class and use them as predictors in another regularized multinomial logistic regression with the other predictors being the bigram tf-idfs.
  
  This model has the following metrics:
  
  - Accuracy: 0.80
  - Sensitivity: 0.665
  - Specificity: 0.936
  
I believe that Caitlyn also had a NN model with similar metrics so we should also mention that.