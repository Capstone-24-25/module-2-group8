---
title: "Summary of exploratory tasks"
author: 'Joshua Charfauros, Jade OBrien, Caitlyn Vasquez, Shirley Wang'
date: today
---

### HTML scraping

Our first task was to change the HTML scraping to include header information as well as paragraph information. To achieve this, we updated the preprocessing script to include headers from our HTML files. We then fitted a binary regularized logistic model using word tf-idfs. This binary model has an accuracy of .786, a sensitivity of .797, a specificity of .776 and an AUC of .851. Comparing this to the binary model that we produced in our in class activity, we see improvement in all areas except for specificity. The in class model has an accuracy of .721, a sensitivity of .621, specificity of .830 and AUC of .796. Interpreting these figures, our binary model incorrectly predicts true negatives 5.4% more of the time, but performs better in all other categories. As for the effect of logistic principal component regression, most of the time it will improve binary class predictions. Logistic principal component regression, reduces the negative effects of multicollinearity by transforming the predictors into uncorrelated principal components. This is effectively reducing the amount of random noise, and only focusing on components that explain the most variation. Logistic principal component regression also helps to mitigate overfitting risks that regular logistic regression may have.

### Bigrams

Our second task was to re-tokenize our data to obtain bigrams, fit a model to the word-tokenized data, then feed this into a second model to analyze the effect of using bigrams. To achieve this, we extracted the predicted log odds of each HTML document, and then combined this feature with bigram tf-idfs features into one model. The resulting logistic regression model has an accuracy of .81, a sensitivity of .695, specificity of .910, and AUC of .909. Comparing this with our model from task 1, we see improvement in all metrics except for sensitivity. Our group is not sure why this occurred. Interpreting these figures, our task 2 model correctly predicts true positives 10.2% less of the time, which admittedly is not great. However, the improvement across all other metrics, specifically a nearly 15% increase in specificity, leads me to believe that our task 2 model is better than task 1. Because of the overall improvement, I would argue that bigrams do capture additional information about the claims status of a page. It seems like most of the information has to do with identifying the negative cases (specificity). By using bigrams, our model has become significantly better at determining when a website does not have useful information on it. 


